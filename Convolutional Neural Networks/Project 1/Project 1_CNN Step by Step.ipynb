{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 - CNN step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Forward Step **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Zero Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line)\n",
    "    # X_pad = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    X_pad= np.pad(X, ((0,0), (pad,pad), (pad,pad),(0,0)), 'constant')\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 7, 7, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x287d0298f50>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1,1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolution operation for one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    #(≈ 3 lines of code)\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    # s = None\n",
    "    # Sum over all entries of the volume s.\n",
    "    # Z = None\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    # Z = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    Z=np.sum(W * a_slice_prev) + b\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[[-6.99908945]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Conv_forward Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad'] \n",
    "    \n",
    "    # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n",
    "    n_H = int( (n_H_prev - f + (2 * pad)) / stride ) + 1\n",
    "    n_W = int( (n_W_prev - f + (2 * pad)) / stride ) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros(shape=(m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "\n",
    "    A_prev_pad = np.pad(A_prev,( (0,0) , (pad,pad) , (pad,pad) ,(0,0) ))\n",
    "    \n",
    "    for i in range(m):               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]               # Select ith training example's padded activation\n",
    "        for h in range(n_H):           # loop over vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h * stride\n",
    "            vert_end = h * stride + f\n",
    "            \n",
    "            for w in range(n_W):       # loop over horizontal axis of the output volume\n",
    "                # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w *stride\n",
    "                horiz_end = w *stride + f\n",
    "                \n",
    "                for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
    "                                        \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
    "                    weights = W[:,:,:,c]\n",
    "                    biases = b[:,:,:,c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev,weights,biases)\n",
    "\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.6923608807576933\n",
      "Z[3,2,1] =\n",
      " [-1.28912231  2.27650251  6.61941931  0.95527176  8.25132576  2.31329639\n",
      " 13.00689405  2.34576051]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\www.Amirpc.com\\AppData\\Local\\Temp\\ipykernel_18204\\2390354288.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z[i, h, w, c] = conv_single_step(a_slice_prev,weights,biases)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4)\n",
    "W = np.random.randn(3,3,4,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\\n\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\\n\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pooling Layer: Max Pooling and Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "\n",
    "       # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i , vert_start:vert_end , horiz_start:horiz_end , :]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. \n",
    "                    # Use an if statement to differentiate the modes. \n",
    "                    # Use np.max and np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[1.74481176 1.74481176 1.74481176]\n",
      "   [1.74481176 1.74481176 1.74481176]\n",
      "   [1.74481176 1.74481176 1.74481176]]\n",
      "\n",
      "  [[2.10025514 2.10025514 2.10025514]\n",
      "   [1.65980218 1.65980218 1.65980218]\n",
      "   [1.6924546  1.6924546  1.6924546 ]]\n",
      "\n",
      "  [[2.18557541 2.18557541 2.18557541]\n",
      "   [2.18557541 2.18557541 2.18557541]\n",
      "   [2.18557541 2.18557541 2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.19891788 1.19891788]\n",
      "   [1.2245077  1.2245077  1.2245077 ]\n",
      "   [1.2245077  1.2245077  1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 1.96710175 1.96710175]\n",
      "   [1.96710175 1.96710175 1.96710175]\n",
      "   [1.62765075 1.62765075 1.62765075]]\n",
      "\n",
      "  [[1.96710175 1.96710175 1.96710175]\n",
      "   [1.96710175 1.96710175 1.96710175]\n",
      "   [1.62765075 1.62765075 1.62765075]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[-0.12321458 -0.12321458 -0.12321458]\n",
      "   [-0.03614932 -0.03614932 -0.03614932]\n",
      "   [ 0.15881017  0.15881017  0.15881017]]\n",
      "\n",
      "  [[-0.06507995 -0.06507995 -0.06507995]\n",
      "   [ 0.05510967  0.05510967  0.05510967]\n",
      "   [ 0.18073215  0.18073215  0.18073215]]\n",
      "\n",
      "  [[ 0.15863244  0.15863244  0.15863244]\n",
      "   [ 0.03434459  0.03434459  0.03434459]\n",
      "   [ 0.21310407  0.21310407  0.21310407]]]\n",
      "\n",
      "\n",
      " [[[-0.0641793  -0.0641793  -0.0641793 ]\n",
      "   [-0.03984495 -0.03984495 -0.03984495]\n",
      "   [ 0.04979996  0.04979996  0.04979996]]\n",
      "\n",
      "  [[ 0.04398565  0.04398565  0.04398565]\n",
      "   [ 0.01118852  0.01118852  0.01118852]\n",
      "   [ 0.0233857   0.0233857   0.0233857 ]]\n",
      "\n",
      "  [[ 0.08033672  0.08033672  0.08033672]\n",
      "   [ 0.10404558  0.10404558  0.10404558]\n",
      "   [ 0.14703955  0.14703955  0.14703955]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[1.74481176 1.74481176 1.74481176]\n",
      "   [1.74481176 1.74481176 1.74481176]]\n",
      "\n",
      "  [[2.18557541 2.18557541 2.18557541]\n",
      "   [2.18557541 2.18557541 2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.19891788 1.19891788]\n",
      "   [1.2245077  1.2245077  1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 1.96710175 1.96710175]\n",
      "   [1.62765075 1.62765075 1.62765075]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[-0.12321458 -0.12321458 -0.12321458]\n",
      "   [ 0.15881017  0.15881017  0.15881017]]\n",
      "\n",
      "  [[ 0.15863244  0.15863244  0.15863244]\n",
      "   [ 0.21310407  0.21310407  0.21310407]]]\n",
      "\n",
      "\n",
      " [[[-0.0641793  -0.0641793  -0.0641793 ]\n",
      "   [ 0.04979996  0.04979996  0.04979996]]\n",
      "\n",
      "  [[ 0.08033672  0.08033672  0.08033672]\n",
      "   [ 0.14703955  0.14703955  0.14703955]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: stride of 2\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ** (Optional) Backward Step **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\www.Amirpc.com\\AppData\\Local\\Temp\\ipykernel_18204\\2390354288.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z[i, h, w, c] = conv_single_step(a_slice_prev,weights,biases)\n"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "        \n",
    "    mask = x == np.max(x)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from shape\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value\n",
    "    a = np.ones(shape) * average    \n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass of the pooling layer\n",
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "       \n",
    "    # Retrieve information from cache\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev\n",
    "                        a_prev_slice = a_prev[vert_start: vert_end, horiz_start: horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                           \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
